## 优化算法

**一阶方法**：随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam。

**二阶方法**：牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS。

**自适应优化算法**：Adagrad（累积梯度平方），RMSProp（累积梯度平方的滑动平均）、Adam（带动量的RMSProp，即同时使用梯度的一、二阶矩））

**梯度下降陷入局部最优有什么解决办法？** 可以用BGD、SGD、MBGD、momentum，RMSprop，Adam等方法来避免陷入局部最优。

### 1、梯度下降算法

#### **①批量梯度下降（BGD）**

批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用**所有的样本**来进行更新

> **优点**：（1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
>
> ​			（2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。
>
> **缺点**：（1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。
>
> ​			（2）不能投入新数据实时更新模型。

#### **②随机梯度下降（SGD）**

随机梯度下降法求梯度时选取**一个样本**j来求梯度。

> **优点：**（1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮**参数的更新速度大大加快**。
>
> **缺点：**（1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧**无法做到线性收敛**。（2）可能会收敛到**局部最优**，由于**单个样本并不能代表全体样本的趋势**。（3）不易于并行实现。SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。

#### **③小批量梯度下降算法（mini-batch GD）**

小批量梯度下降法是是对于m个样本，我们采用x个子样本来迭代，1<x<m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。

> 优点：（1）每次在一个batch上优化神经网络参数并不会比单个数据慢太多。
>
> ​			（2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)
>
> ​			（3）可实现并行化。
>
> 缺点：lr和batch_size不好选择

### 2、梯度下降算法改进

**①动量梯度下降法（Momentum）**

Momentum 通过加入 γ*vt−1 ，可以加速 SGD， 并且抑制震荡。

![image-20220320200857052](C:%5CUsers%5CBreeze%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220320200857052.png)

上式中第一行表示vt受到前一个vt的影响，r通常取值0.9，表示动量，再加上当前梯度（第一行后一项）得到动量梯度

**②Adagrad**

![image-20220320201427086](C:%5CUsers%5CBreeze%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220320201427086.png)

由上图最后一个式子知，Adagrad的学习率是动态变化的

> **优点**：减少了学习率的手动调节。超参数设定值：一般η选取0.01。
>
> **缺点**：它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小。