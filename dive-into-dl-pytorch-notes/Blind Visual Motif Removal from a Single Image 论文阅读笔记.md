## Blind Visual Motif Removal from a Single Image 论文阅读笔记

杂杂的

> 传统visual motif remove 的方法通常是分为两个步骤，先检测水印位置再进行去除，依赖于破坏像素位置信息，本文首次提出blind端到端方法进行motif水印去除，将corrupted image分解为background和visual motif的方法更有利于重建图像，这是受到了“Generative single image reflection separation ”的启发（去除反射）
>
> 数据集：由coco合成
>
> 网络结构：借鉴了UNet，有一个encoder，三个decoders
>
> **Unet**
>
> **Residual**
>
> **Transpose Convolution**
>
> **上采样有3种常见的方法：双线性插值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling)**
>
> 卷积尺寸计算公式：`o = (i + 2*p + s - k)/2`
>
> 反向卷积尺寸计算公式：
>
> > ```python
> > if (o + 2*p - k) % s == 0:  
> > 	o = s*(i - 1) - 2*p + k
> > else:
> > 	o = s*(i - 1) - 2*p + k + (o + 2*p - k) % s
> > ```
>
> **hard example mining**
>
> inspiration -- [15]
>
> [5] spatial perturbation

**翻译：**

[TOC]

## Abstract

许多在互联网上分享的图片都包含一些可见的水印图像，例如文本、标志或者是绘图，这为图片增加了描述和装饰。例如，装饰性的文字可以指明图像的拍摄地点。通常来说，重复出现的水印在语义上是相似的，但在位置、风格和内容上是不同的。本文提出一种基于深度学习的技术来“盲”去除这些水印对象。在“盲”的定义下，水印的位置和确切的形状都是未知的。我们的方法能够在预测出哪些像素包含水印图像的同时综合分析其潜在的修复图像。它可以应用于单个输入图像，没有任何用户的帮助来指定水印的位置，达到了“盲”去除不透明和半透明水印图像的最先进效果

## 1. Introduction

针对各种目的，互联网上的图像通常包含一些水印。例如，添加大学的徽章或者团队的logo可以使得一个图像充满自豪感，或者是一个带有温度的信息也可以增加一张图像额外的信息。在一些情况下，水印还被用于保护图像所有权，例如数字水印。然而，在许多情况下，人们可能想要去除这些水印，并恢复原有的图像。

去除这些水印和恢复一个原始图像可能是一个极具挑战性的任务。在不同的图像中，这些物体的结构、大小和位置各不相同，如果没有用户的指导或对底层图像的假设，就很难检测到它们。之前的方法都是依赖于被破坏像素的位置信息来恢复[21,20,24,36]。Dekel等人的[5]使用包含相同水印的大型图像集合，以及关于水印位置的最小用户指导来去除水印。

我们提出了一种完全“盲”的水印去除方法。在盲的定义中，这些图案的确切位置、结构和大小是未知的。我们的网络的泛化能力是通过去除训练中看不到的视觉图案来证明的。

与以前的方法不同，我们的策略不需要多个有着相同需要去除目标的图像[5]，也不需要水印像素的确切位置。我们的技术利用了深度神经网络的能力，将训练过程中看到的样本归纳为测试时间中不可见的样本，从而能够去除新的(即没见过的)水印。该网络学习从图像中分离水印，首先估计包含水印的像素，然后重建潜在的图像。除了估计二值化水印的轮廓和重建潜在图像外，我们还重建了可见水印图像。

将输入图像分解为水印图像和背景图像，使网络能够更好地重建图像。在我们的实验中，我们证明了被不同类型的视觉图案(如文本或半透明的图画)损坏的图像可以成功地恢复。

## 2. Related work

虽然目前没有文献直接讨论视觉图案的“盲”去除问题，但在水印去除和盲图像修复领域有相关的研究。

### 水印去除

数字水印通常用于识别图像的版权，以防止用户在未经同意的情况下使用在线图像。虽然有许多创建水印的技术可用[27]，但是可见水印的创建过程通常是由嵌入不同透明度的logo或文本到目标图像中组成的。

水印攻击方法，如[29]，力求从图像中准确地去除水印。水印对攻击的鲁棒性是保护图像版权的关键。为了测试和提高水印的复原能力，研究了水印去除方法。去除水印通常包括两个步骤:水印检测和底层图像内容的重建。

大多数水印去除方法都是通过对单个水印图像的重构来识别水印的位置。Huang和Wu[11]从用户处获取水印位置，然后使用图像补画恢复潜图像。Pei和Zeng[21]利用独立分量分析(ICA)将源图像与水印分离。Park等人的[20]找到一种颜色空间变换，将源图像与水印分离。最近，Dekel等人的[5]假设存在一组具有相同水印的图像来估计和去除它。与这些方法相反，我们的方法不需要任何用户的干预，水印可以在只使用单个图像的条件下就能够被去除。

### 图像修复

图像修复是恢复或完善图像部分的过程[1,2,3,4,22]。大多数图像补绘方法都不是“盲”的，因为它们假定要恢复的像素的二值掩模是给定的[10,12,28,32,33]。

有些工作考虑了“盲”补的情况，但在盲补的情况下，也还需要检测待补像素的掩膜。Zoran  et  al.[36]提出了一种基于GMM的模型来估计二值掩膜和恢复潜在图像。其他方法使用具有“盲”和“非盲”设置的深度神经网络，可用于恢复潜图像[18,24]。显然，在任何情况下，如果不使用缺失像素的遮罩，盲目地进行补画，性能都会下降。

可以使用盲补水印去除，但这不会考虑到水印所提供的潜在图像信息的半透明性。从这个意义上说，它不如使用这些可用信息（非“盲”）的方法。

### 图像重建

其他之前的与水印去除相关的工作包括去除雨滴[6,14,23,34]、分离目标[9]和去除图像中的反射[8,31,35]。我们的工作灵感来自于反射分离的工作[15]，它将一个反射的图像分离成两个清晰的图像。与这种方法相反，我们使用分离来从图像中去除水印图案。通过我们的方法删除的元素可以是不同大小的。与整个图像相比，它们可能非常小，因此很难检测到。

## 3、Method

从单个图像中移除水印是一项很难实现的任务，因为没有事先了解水印图像。我们提出的方法使用训练有素的卷积神经网络(CNN)来去除嵌入在图像中的水印来解决这个问题。我们使用带有水印的合成图像数据集来训练网络(参见图4)。

我们的网络学习从图像中分离水印，通过估计水印边缘和重建潜在的图像。在训练过程中，损失计算使用输入图像和视觉母题作为地面真相来训练编码器和解码器网络。我们的网络将带水印的图像编码成一个潜在的表示，由三个并行的解码器分支进行解码:一个用于估计潜在图像，一个用于估计水印二值化掩膜，另一个用于估计水印图像。最终的图像是通过使用估计的水印二值化掩膜从输入图像或重建图像中选择像素生成的。参见图2以获得概述。

### 3.1 嵌入水印

![image-20220323145714104](C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323145714104.png)

Cr 表示corrupt，即含水印的破坏图像，α 表示水印透明度

恢复潜像相当于图像补绘。在[5]的工作之后，我们还通过在抠图之前对主题像素应用空间扰动来试验更具挑战性的主题。为了恢复潜像，α和视觉motif V  m都必须已知。

### 3.2 “盲”水印去除

我们的baseline由一个编码器和两个解码器分支组成（image decoder，mask decoder），最后的修复图像是通过：

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323150430207.png" alt="image-20220323150430207" style="zoom:80%;" />

损失函数由两部分组成：

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323150539617.png" alt="image-20220323150539617" style="zoom:80%;" />

其中对loss mask使用binary cross entropy：

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323150608741.png" alt="image-20220323150608741" style="zoom:67%;" />

对修复后的图像采用L1距离：

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323150730245.png" alt="image-20220323150730245" style="zoom:50%;" />

其中|Ma|表示mask的面积，

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323150848305.png" alt="image-20220323150848305" style="zoom:60%;" />

注意这里预计Pix采用的是真值Ma而不是预测的Ma，目的是分离两个解码器的损失项，否则会导致过拟合。

当然，在某些情况下，我们的需要重建的可能是水印图像，在这种情况下，我们为推测原始水印图像再添加一个解码器分支visual motif decoder。这个分支可以得到原始水印的输出，并且与image decoder在前几层有共享权重。在后面的消融实验中我们展示了多加入一个这样的分支不仅能够得到原始水印的修复，还能带来图像修复效果的略微提升。

（第三个解码器的损失函数也类似采用L1距离，不在赘述）

### 3.3 网络结构

基于U-Net架构，将解码器层的特征图与相应的编码器层的特征图在相同分辨率下进行组合（跳跃连接）。

图3显示了体系结构的详细信息。编码器和解码器包含五个主要部分。在编码器中，每个段将信道数量增加到原来的2倍。它包括3 × 3卷积层，批处理归一化[13]和ReLU激活单元[19]。使用带有stride-2的max pooling减小了输入的空间大小。

解码器执行逆运算。每个解码段扩展输入的空间大小，并将通道数量减少到原来的2倍。它从一个带有stride-2的3 × 3 反卷积层开始。然后，在前面的输出和相应的编码数据上应用一个额外的卷积层。批量归一化和ReLU也被使用。在解码器的最后一段之后，通过应用1 ×  1卷积层生成最终输出，然后对掩码解码器进行sigmoid激活，对图像和视觉motif解码器进行tanh激活。注意，在编码器和解码器中，每个段中都使用了三个residual block。这扩大了接收域，并提高了恢复图像的质量(参见第4.3节)。

## 4. Experiments

我们展示了我们的方法在各种类型的水印图案上的结果: 彩色和灰色文本，表情符号[7]和几何形状。我们为每种类型的视觉主题训练了不同的网络。在我们所有的实验中，我们的背景图像由2700训练图像和另外300张测试图像组成，它们是随机选自Microsoft COCO val2014数据集的。

### 4.1 透明水印

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323162451411.png" alt="figure-4" style="zoom: 80%;" />

#### 训练

为了训练网络，我们合成了几组含水印的图像(如图4所示)。对于每种类型，我们合成了2万幅512 × 512的水印图像，各个类型如下：

(i) 带有随机字体和颜色的拉丁字符的彩色文本。在每个图像中，使用固定的不透明度α∈(0.3,0.7)将这些图像混合到背景图像中。

(ii) 灰度文本的特点是随机的浅色和深色边框。它们嵌入了一个均匀分布的不透明度混合域，在常数α 周围有10%的方差，以及一个高达一个像素偏移的随机空间扰动。

(iii) emojis数据集有800个随机选择的对象。它们在α∈(0.4,0.6)范围内以恒定的不透明度混合，并应用相同的扰动和10%的不透明度方差。

(iiii) 直线、矩形和椭圆等几何灰度形状;在α∈(0.2,0.9)范围内，它们的不透明度是恒定的。

对于每个训练图像，我们使用随机位置、比例、作物和旋转嵌入多达10个视觉主题，以增强数据。为了加快训练速度，我们使用从损坏的图像中随机选取大小为128 ×  128的patches作为输入到网络中(见图4)，但测试的是全分辨率的图像。

#### 测试

测试图像由全分辨率图像组成，这些图像在训练中是不可见的。对于文本实验，我们训练的是拉丁字符，但水印是来自其他语言的未曾见过的字符，例如，汉语、日语、印地语或格鲁吉亚语。表情符号测试集包含了训练中没有使用过的表情符号。各种重建结果如图1和图5所示，以及在补充材料中。为了突出测试集和训练集之间的差异，我们在补充材料中展示了几个测试表情符号(例如，图1和图5中出现的表情符号)和它们的5个欧几里德最近邻。

#### 单独水印去除

我们将我们的方法与四种算法的性能进行了比较:自然图像抠图(**CFM**)的封闭解[16]、多图像重建(**MMR**)[5]和两种深度图像反射分离方法(**SIRF**,  **BDN**)[31,35]，我们将其用于水印去除问题。为了便于MMR任务，我们在整个测试集(共1000张图片)上只使用一个看不见的表情符号作为叠加的视觉主题。为了方便CFM的任务，我们提供了ground-truth  motif混合图像。

我们的测试图像被分成三组，伴随着抠图变形水平的增加。在第一组，视觉主题有一个固定的大小160  ×  160像素。它出现在一个随机的位置，并在每个图像α∈(0.4,0.6)内以恒定的不透明度进行混合。对于第二组，我们应用添加扰动的混合到视觉主题的边缘和变化的不透明度。对于第三组，增加了大小和旋转的变化。视觉母题的大小为120×120−200×200  px，在θ∈(−20◦，20◦)之间随机旋转。

通过比较PSNR和结构相似度指数(SSIM)[30]条件下的重建图像与地面真实图像的对比结果来衡量。我们的方法在更具有挑战性的场景下具有更好地性能。

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323162548853.png" alt="Table-1" style="zoom:50%;" />

小结：反射光的分离工作遵循编码器解码器框架，因为它们必须推广到可能出现在**整个图像中的反射**。而图像中反射光和水印去除的主要区别是**透明像素的分布**。水印图像将始终包含来自背景图像的语义。由于我们的方法是专门针对水印图案的，我们限制了二值水印掩膜的重建。在消融实验中，我们展示了仅使用常规编码解码器的效果是差于使用motif mask branch的。

#### 水印去除

对于这个任务，我们结合上面列出的不同训练集来训练网络。具体来说，我们使用了合成光文本集(ii)和灰度形状集(iv)，以及合成白文本数据集。这些文本使用不变的不透明度α∈(0.2,0.7)嵌入，然后使用3  × 3高斯模糊滤波器，得以使得水印和背景图像很好的融合。

### 4.2 修复

我们测试的另一种类型的水印是盲修补任务:网络对背景图像或损坏区域的掩模没有明确的先验。我们在两个层次上评估我们的网络。

第一个是由256 ×  256像素的图像组成，我们在这些图像上平铺了字体为Hel vetica  light的随机黑色文本，并随机进行旋转，每个单词的大小在80到120像素之间。

在第二个实验中，我们使用了(黑色)粗体的Helvetica中等字体，每个字高达150像素。对于这个任务，我们在没有Vm分支的情况下训练了我们的baseline配置，因为检索像素掩码相当于检索黑色区域。我们对同样的2700张图像进行训练，随机裁剪成64 ×  64像素的patch。每个测试数据集包含100个未见过的图像和未见过的单词。

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323165740086.png" alt="image-20220323165740086" style="zoom: 50%;" />

表2总结了每次实验对100幅试验图像的定量结果。

![image-20220323165820335](C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323165820335.png)

图7显示了与其他方法的可视化比较。结果表明，我们的方法在完成真实图像模式的任务中表现得更好，在填充较大的损坏区域时，得到了更清晰的结果。我们也在这些方法的样本图像上使用了我们预先训练好的网络(见补充资料)。即使使用他们的样本图像，我们的方法仍然优于其他方法。

### 4.3 消融实验

在我们的消融研究中，我们通过测试不同分支配置和内部结构设置所获得的结果的质量来验证我们的网络配置。

如前所述，我们为研究生成训练数据集，使用表情符号[7]作为尺寸为256  ×  256像素的图像的视觉图案。主题大小是均匀随机采样范围(80,160)像素。我们用随机抽样∈(0.3,0.7)的混合常数α混合母题。每个测试网络在大小为128  × 128的patch上训练100 epoch。我们在200幅看不见的图像和看不见的图案上测试了这些网络的性能。

在网络配置方面，我们比较包括auto-encoder  U-Net模型，我们的baseline模型，两个分支的三个分支模型分离的分支主题(基线+ Vm)和含三个分支的可选模型(基线+共享Vm)以及共享权重的设置。

表3总结了测试集的定量结果(可视化比较在补充材料中)。结果表明，在没有掩码分支的情况下，自动编码器网络不能将图案与背景图像分离。结构采用： (Baseline + shared  Vm)时，效果最佳。此外，我们还探讨了image和motif分支之间共享权值大小的影响，并通过改变每个编码器-解码器段内的剩余块数量，尝试进行较浅和较深的设置。表4中这些研究的定量结果表明，添加(最多三个)res-block和使用共享权值(最多两个解码级别)改善了图像的重建。

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323170658165.png" alt="image-20220323170658165" style="zoom:67%;" />

## 5. Conclusion

我们提出了一种识别和去除嵌入图像中的视觉图案的方法。我们的方法是第一个从图像中“盲”去除视觉主题(“盲”：即，没有明确的先验信息)。需要强调的是，与以前的方法不同，我们的方法不需要任何用户输入，也不需要对视觉主题做出复杂的假设。

深度神经网络能够泛化超出训练中给出的例子。这使我们能够在一组视觉图案上进行训练，并在测试期间移除大量其他视觉图案，这些图案在大小、位置、颜色甚至形状上可能与训练集有显著差异。这种泛化能力在训练拉丁字符时成功地去除了印度、中国和日本字符中得到了明显的体现(如图1和图5)。

我们的方法的一个显著的优势是增加了一个解码器网络来重建分散注意力的视觉主题。我们已经证明，这导致了一个更好的潜在图像重建和视觉motif检测。这在移除半透明的视觉图案时尤为显著，这些图案嵌入了不同的透明度或小的空间扰动。

缺点：当水印嵌入在一个统一的背景时，去除后水印的痕迹仍有些明显。当主题嵌入平坦的统一区域时，我们的方法可能会遇到困难。我们认为这种限制可以通过hard example mining训练来解决。

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220323171232028.png" alt="image-20220323171232028" style="zoom: 50%;" />

另一个有趣的未来方向是添加生成网络来合成复杂的视觉图案，如雨滴或其他视觉干扰物。