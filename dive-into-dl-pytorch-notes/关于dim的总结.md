## 关于dim的总结

python很多函数里面都会有dim这个参数，最基本的理解是对于二维矩阵，dim=0表示行，dim=1表示列，对二维Mat im（h, w）,求和im.sum(dim=0)，将得到输出im_out (1, w)。

:one:首先mat的shape可以通过输出的形状数出来，

```python
>>> x
tensor([[[0.3384, 0.3919, 0.9118, 0.7277, 0.7072],
         [0.4396, 0.9642, 0.3606, 0.1438, 0.3807],
         [0.6454, 0.9058, 0.2737, 0.5792, 0.6238],
         [0.2874, 0.3826, 0.8367, 0.4606, 0.4972],
         [0.9586, 0.8566, 0.9053, 0.2073, 0.1934]],

        [[0.9452, 0.3448, 0.8671, 0.7115, 0.2633],
         [0.1935, 0.8044, 0.6325, 0.8062, 0.6648],
         [0.4996, 0.3297, 0.1910, 0.1676, 0.8024],
         [0.7707, 0.9455, 0.2666, 0.9008, 0.5115],
         [0.1561, 0.6539, 0.3266, 0.3890, 0.0026]],

        [[0.2800, 0.8999, 0.5534, 0.1352, 0.7807],
         [0.1994, 0.9092, 0.7731, 0.8702, 0.7340],
         [0.0886, 0.5077, 0.6287, 0.3569, 0.6537],
         [0.3910, 0.5635, 0.8676, 0.3961, 0.0041],
         [0.1527, 0.0085, 0.6075, 0.7849, 0.7485]]])
# 数逗号，从最里层单个元素开始，最里层被直接包围的一个方括号里面有几个逗号+1就是最后一维shape值（5）
# 第二维shape值（5）
# 第一维shape值（3）
# 即m.shape = (3,5,5)
```

:two:简化为二维mat：

```python
>>> im = torch.rand(4,5) #四行五列
>>> im
tensor([[0.5442, 0.1621, 0.0666, 0.4127, 0.2133],
        [0.9653, 0.4114, 0.9162, 0.3221, 0.1214],
        [0.6930, 0.3620, 0.1851, 0.7593, 0.2037],
        [0.3621, 0.0602, 0.4252, 0.8884, 0.1334]])
>>> im.sum(dim=0) #加和成一行
tensor([2.5647, 0.9958, 1.5932, 2.3826, 0.6717])
>>> im.sum(dim=0).shape
torch.Size([5])
>>> 
>>> im.sum(dim=0,keepdim=True) # keepdim可以保持mat原本形状，否则指定dim的维度会“消失”
tensor([[2.5647, 0.9958, 1.5932, 2.3826, 0.6717]])
>>> im.sum(dim=0,keepdim=True).shape
torch.Size([1, 5])
>>> 
```

可以理解为，若dim=i，则第i+1维的shape会变0，若keepdim了则是变1。

:three:三维的例子

```python
>>> x = torch.rand(3,5,5)
>>> x
tensor([[[0.3384, 0.3919, 0.9118, 0.7277, 0.7072],
         [0.4396, 0.9642, 0.3606, 0.1438, 0.3807],
         [0.6454, 0.9058, 0.2737, 0.5792, 0.6238],
         [0.2874, 0.3826, 0.8367, 0.4606, 0.4972],
         [0.9586, 0.8566, 0.9053, 0.2073, 0.1934]],

        [[0.9452, 0.3448, 0.8671, 0.7115, 0.2633],
         [0.1935, 0.8044, 0.6325, 0.8062, 0.6648],
         [0.4996, 0.3297, 0.1910, 0.1676, 0.8024],
         [0.7707, 0.9455, 0.2666, 0.9008, 0.5115],
         [0.1561, 0.6539, 0.3266, 0.3890, 0.0026]],

        [[0.2800, 0.8999, 0.5534, 0.1352, 0.7807],
         [0.1994, 0.9092, 0.7731, 0.8702, 0.7340],
         [0.0886, 0.5077, 0.6287, 0.3569, 0.6537],
         [0.3910, 0.5635, 0.8676, 0.3961, 0.0041],
         [0.1527, 0.0085, 0.6075, 0.7849, 0.7485]]])
>>> x.shape
torch.Size([3, 5, 5])

# dim = 0, 输出第一个shape值为1
>>> x.mean(dim=0,keepdim=True)
tensor([[[0.5212, 0.5455, 0.7774, 0.5248, 0.5837],
         [0.2775, 0.8926, 0.5887, 0.6067, 0.5932],
         [0.4112, 0.5811, 0.3645, 0.3679, 0.6933],
         [0.4830, 0.6305, 0.6570, 0.5859, 0.3376],
         [0.4225, 0.5063, 0.6131, 0.4604, 0.3148]]])
>>> x.mean(dim=0,keepdim=True).shape  # dim = 0, x0的第一个shape值为1
torch.Size([1, 5, 5])

# dim = 1, 输出第二个shape值为1
>>> x.mean(dim=1,keepdim=True)
tensor([[[0.5339, 0.7002, 0.6576, 0.4237, 0.4805]],

        [[0.5130, 0.6157, 0.4568, 0.5950, 0.4489]],

        [[0.2223, 0.5778, 0.6861, 0.5086, 0.5842]]])
>>> x.mean(dim=1,keepdim=True).shape  # dim = 1, 输出第二个shape值为1
torch.Size([3, 1, 5])

# dim = 2, 输出第三个shape值为1
>>> x.mean(dim=2,keepdim=True)
tensor([[[0.6154],
         [0.4578],
         [0.6056],
         [0.4929],
         [0.6242]],

        [[0.6264],
         [0.6203],
         [0.3980],
         [0.6790],
         [0.3056]],

        [[0.5298],
         [0.6972],
         [0.4471],
         [0.4444],
         [0.4604]]])
>>> x.mean(dim=2,keepdim=True).shape  # dim = 2, 输出第三个shape值为1
torch.Size([3, 5, 1])
```

:four:由此就可以理解batch normalization中求mean的mat操作:

` mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)`

**对于一个batch内的n个样本（n=2），每个样本有三个通道（rgb）(c=3)，每个通道有行和列(h=5, w=5), 在conv_BN中，需要在一个batch里求mean，且每个通道的参数相互独立，因此唯独没有对dim=1求mean**

```python
>>> x_4 = torch.rand(2,3,5,5) # mat(n,c,h,w)
>>> x_4
tensor([[[[0.4608, 0.9335, 0.8002, 0.4690, 0.7002],
          [0.3885, 0.4306, 0.5764, 0.1270, 0.9433],
          [0.8589, 0.6746, 0.1805, 0.9029, 0.9895],
          [0.5164, 0.4216, 0.9904, 0.4707, 0.2062],
          [0.1222, 0.3489, 0.7620, 0.7344, 0.1182]],

         [[0.1689, 0.0233, 0.4864, 0.9040, 0.0605],
          [0.0448, 0.4667, 0.5443, 0.8003, 0.3209],
          [0.1356, 0.6696, 0.4991, 0.2553, 0.6510],
          [0.5681, 0.8360, 0.7013, 0.9838, 0.5886],
          [0.6585, 0.0767, 0.3724, 0.5822, 0.7495]],

         [[0.4381, 0.4650, 0.0284, 0.3267, 0.4024],
          [0.4368, 0.6771, 0.5633, 0.8237, 0.5246],
          [0.0489, 0.8678, 0.2774, 0.2327, 0.5241],
          [0.2637, 0.8721, 0.9914, 0.9337, 0.9016],
          [0.2763, 0.1742, 0.3884, 0.9695, 0.6642]]],


        [[[0.7491, 0.0306, 0.1089, 0.4010, 0.9611],
          [0.3613, 0.9215, 0.6076, 0.5056, 0.7323],
          [0.2528, 0.0953, 0.7099, 0.2004, 0.6717],
          [0.4343, 0.2031, 0.0581, 0.9414, 0.8554],
          [0.7249, 0.4760, 0.9487, 0.6846, 0.5735]],

         [[0.3409, 0.5301, 0.5996, 0.8978, 0.8446],
          [0.2794, 0.9100, 0.6648, 0.5530, 0.9234],
          [0.8472, 0.6638, 0.5295, 0.9106, 0.0330],
          [0.8847, 0.4928, 0.0036, 0.6927, 0.3521],
          [0.6128, 0.7455, 0.2994, 0.1516, 0.0782]],

         [[0.2109, 0.2473, 0.1858, 0.4634, 0.7542],
          [0.8290, 0.7766, 0.6246, 0.1593, 0.6657],
          [0.3720, 0.3073, 0.6639, 0.9781, 0.8433],
          [0.1746, 0.5683, 0.2349, 0.8420, 0.8367],
          [0.0241, 0.9178, 0.7445, 0.1893, 0.7605]]]])
# 每个通道的参数相互独立，因此唯独没有对dim=1求mean
>>> x_4.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)
tensor([[[[0.5467]],

         [[0.5198]],

         [[0.5289]]]])
```

