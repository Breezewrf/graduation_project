# 常用专业名词解释

[TOC]

### batch_size

 		每个批次(batch)的样本数量

### epoch

​		所有样本一共要计算epoch遍

### step即iteration

​		一个step/iteration其实指的就是一次梯度更新的过程；例如在每个epoch中有2000个用于训练的图片，我们选取了batch_size=100，那么我们就需要2000 images / 100 (images/step) = 20 steps来完成这个epoch

## 上采样方法

- U-Net在上采样使用的是转置卷积（也有人称反卷积），而FCN上采样使用的是向上池化操作。

### 插值法

线性插值、二次插值等，需要经过人工挑选，不具有可学习性，效果一般

### 转置卷积

[Up-sampling with Transposed Convolution | by Naoki | Medium](https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)

[(翻译) 一文搞懂反卷积，转置卷积_FesianXu的博客-CSDN博客](https://blog.csdn.net/LoseInVain/article/details/81098502)

也被称作反卷积（deconvolution），但是用“转置”一词更能够表示它的涵义

**首先理解转置卷积需要引入卷积矩阵（Convolution Matrix）的概念**

> 先将卷积核进行矩阵化：
>
> <img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220329101750852.png" alt="image-20220329101750852" style="zoom: 67%;" />
>
> <img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220329101628004.png" alt="image-20220329101628004" style="zoom:67%;" />
>
> 

> 将图像矩阵化为一个向量
>
> <img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cimage-20220329102041919.png" alt="image-20220329102041919" style="zoom:50%;" />

> 由此有：卷积矩阵[4, 16] * 图像矩阵[16, 1] => 结果矩阵[4, 1]
>
> 可以将结果矩阵reshape成[2, 2]，就实现了一个downsize的操作
>
> 那么反过来，是不是也可以upsize？

> <img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Ctran_Conv.png" alt="Trans_Conv" style="zoom:50%;" />
>
> 我们将卷积矩阵进行转置，再与图像矩阵相乘
>
> 转置卷积矩阵[16, 4] * 图像矩阵[4, 1] => 结果矩阵[16, 1]
>
> 将结果矩阵reshape一下就得到了upsize的图像！
>
> **但值得注意的是，上述两次操作并不是可逆关系，对于同一个卷积核（因非其稀疏矩阵不是正交矩阵），结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状**，所以转置卷积的名字由此而来，相比于“逆卷积”而言转置卷积更加准确。

因此一般称之为**转置卷积**！

在使用转置卷积时观察到一个棘手的现象（尤其是深色部分常出现）就是"棋盘格子状伪影"，被命名为**棋盘效应**（Checkboard artifacts）

如果将步幅设为2，在卷积核大小为2的示例中，输出上的所有像素从输入中接收相同数量的信息。由下图（a）可见，此时描以转置卷积的重叠。若将卷积核大小改为4（下图（b）），则均匀重叠区域将收缩，与此同时因为重叠是均匀的，故仍然为有效输出。

但如果将卷积核大小改为3，步长为2（下图（c）），以及将卷积核大小改为5，步长为2（下图（d）），问题就出现了，对于这两种情况输出上的每个像素接收的信息量与相邻像素不同。在输出上找不到连续且均匀重叠区域。**因此出现了规律性的格子阴影——棋盘效应**。

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cv2-6288d1734ad00c718fc814e4c7bbc985_720w.jpg" alt="img" style="zoom: 80%;" />

​	在二维情况下棋盘效应更为严重，下图直观地展示了在二维空间内的棋盘效应。

<img src="C:%5CUsers%5CBreeze%5CDesktop%5Cgra_proj%5Cgraduation_project%5Cdive-into-dl-pytorch-notes%5Cimages%5Cv2-de1bb8f86193666e3b0a1539d273ab32_720w.jpg" alt="img" style="zoom:67%;" />

解决方法：

采取可以被步长整除的卷积核长度；

先插值，再卷积（用双线性插值初始化转置卷积权重）

> ```python
> def bilinear_kernel(in_channels, out_channels, kernel_size):
> 	factor = (kernel_size + 1) // 2
> 	if kernel_size % 2 == 1:
> 		center = factor - 1
> 	else:
> 		center = factor - 0.5
> 	og = (torch.arange(kernel_size).reshape(-1, 1),
> 		  torch.arange(kernel_size).reshape(1, -1))
> 	filt = (1 - torch.abs(og[0] - center) / factor) * \
> 		   (1 - torch.abs(og[1] - center) / factor)
> 	weight = torch.zeros((in_channels, out_channels, kernel_size, kernel_size))
> 	weight[range(in_channels), range(out_channels), :, :] = filt
> 	return weight
> conv_trans = nn.ConvTranspose2d(
> 							3, 2, kernel_size=4, padding=1, stride=2, bias=False)
> conv_trans.weight,data.copy_(bilinear_kernel(3, 3, 4))
> 
> ```

### Unsample，Unpooling

<img src="./images/16715697-b242bfec49762317.webp" alt="img" style="zoom:50%;" />

<img src="./images/16715697-baa5ce46a7509840.webp" alt="img" style="zoom:50%;" />

即Unsampling和UnPooling都是对Maxpooling操作的像素点对应的位置进行还原

不同在于UnSampling在其他位置采用same策略，UnPooling在其他位置采用补0

